{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from airo_robots.manipulators.hardware.ur_rtde import URrtde\n",
    "from airo_camera_toolkit.cameras.realsense import Realsense\n",
    "from airo_spatial_algebra.se3 import SE3Container\n",
    "import numpy as np\n",
    "import cv2\n",
    "from microplate.opencv_annotation import get_manual_annotations_image, Annotation\n",
    "from microplate.camera_pose import get_camera_pose\n",
    "from airo_camera_toolkit.utils import ImageConverter\n",
    "from microplate.multi_view_triangulation import multiview_triangulation_midpoint, get_triangulation_errors\n",
    "from airo_robots.grippers.hardware.robotiq_2f85_urcap import Robotiq2F85\n",
    "\n",
    "# reload code\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gripper = Robotiq2F85(\"10.42.0.162\")\n",
    "gripper.open().wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_pose_in_tcp = get_camera_pose()\n",
    "camera_pose_in_tcp = get_camera_pose()\n",
    "camera = Realsense(resolution=Realsense.RESOLUTION_720,fps =6)\n",
    "robot = URrtde(\"10.42.0.162\",URrtde.UR3E_CONFIG)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect the views to determine the positions of the keypoints on the microplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "home_pose = SE3Container.from_euler_angles_and_translation([np.pi,0,0],[0.3,0.0,0.2]).homogeneous_matrix\n",
    "robot.move_linear_to_tcp_pose(home_pose,0.1).wait()\n",
    "gripper.open()\n",
    "\n",
    "target_position = np.array([0.34,-0.01,0.0]) # approximate center of the microplate\n",
    "\n",
    "# determine the camera poses for the three views\n",
    "view_position_1 = np.array([0.3,0.0,0.3])\n",
    "view_position_2 = np.array([0.35,-0.1,0.3])\n",
    "view_position_3 = np.array([0.3,0.1,0.3])\n",
    "#view_position_4 = np.array([0.25,-0.2,0.2])\n",
    "\n",
    "view_positions = [view_position_1,view_position_2,view_position_3]\n",
    "tcp_poses = []\n",
    "for view in view_positions:\n",
    "    view_orientation = np.eye(3)\n",
    "    z_vector = target_position - view\n",
    "    z_vector /= np.linalg.norm(z_vector)\n",
    "    x_vector = np.cross(z_vector,np.array([0,0,1]))\n",
    "    x_vector /= np.linalg.norm(x_vector)\n",
    "    y_vector = np.cross(z_vector,x_vector)\n",
    "    y_vector /= np.linalg.norm(y_vector)\n",
    "    view_orientation[:,0] = x_vector\n",
    "    view_orientation[:,1] = y_vector\n",
    "    view_orientation[:,2] = z_vector\n",
    "    camera_pose = SE3Container.from_rotation_matrix_and_translation(view_orientation,view).homogeneous_matrix\n",
    "    tcp_pose = camera_pose @ np.linalg.inv(camera_pose_in_tcp)\n",
    "    tcp_poses.append(tcp_pose)\n",
    "\n",
    "# collect the images from the three viewpoints\n",
    "images = []\n",
    "camera_poses = []\n",
    "for pose in tcp_poses:\n",
    "    robot.move_linear_to_tcp_pose(home_pose,0.1).wait()\n",
    "    robot.move_linear_to_tcp_pose(pose,0.1).wait()\n",
    "    time.sleep(1)\n",
    "    image = camera.get_rgb_image()\n",
    "    images.append(image)\n",
    "    camera_poses.append(pose @ camera_pose_in_tcp)\n",
    "    # cv2.imshow(\"image\",image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "robot.move_linear_to_tcp_pose(home_pose,0.1).wait()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manual annotation of the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # annotate the images\n",
    "# annotation_spec = {\n",
    "#     \"topleft\": Annotation.Keypoint,\n",
    "#     \"topright\": Annotation.Keypoint,\n",
    "#     \"bottomleft\": Annotation.Keypoint,\n",
    "\n",
    "# }\n",
    "\n",
    "# annotations = []\n",
    "\n",
    "# for image in images:\n",
    "#     image_anns = get_manual_annotations_image(image, annotation_spec)\n",
    "#     image_keypoints = []\n",
    "#     for kp in annotation_spec.keys():\n",
    "#         keypoint = image_anns[kp]\n",
    "#         keypoint = np.array(keypoint)\n",
    "#         image_keypoints.append(keypoint)\n",
    "#     annotations.append(image_keypoints)\n",
    "# print(annotations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use neural network to find the keypoints on the microplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keypoint_detection.utils.heatmap import get_keypoints_from_heatmap\n",
    "from keypoint_detection.utils.load_checkpoints import get_model_from_wandb_checkpoint\n",
    "import torch\n",
    "model = \"tlips/microplate-keypoints/model-a0dlcw4n:v10\"\n",
    "device = \"cuda\"\n",
    "model = get_model_from_wandb_checkpoint(model)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the keypoints from the image\n",
    "from airo_camera_toolkit.image_transforms import Resize\n",
    "transform = Resize((1280,720),256,512)\n",
    "annotations = []\n",
    "for image_idx,image in enumerate(images):\n",
    "    print(image.shape)\n",
    "    image = transform(image)\n",
    "    image = torch.Tensor(image.copy()).permute(2, 0, 1).to(device)\n",
    "    with torch.no_grad():\n",
    "        heatmaps = model(image.unsqueeze(0)).squeeze(0)\n",
    "    predicted_keypoints = [\n",
    "        torch.tensor(get_keypoints_from_heatmap(heatmaps[i].cpu(), 2,max_keypoints=1)) for i in range(heatmaps.shape[0])\n",
    "    ]\n",
    "    print(predicted_keypoints)\n",
    "    opencv_image = image.permute(1,2,0).cpu().numpy() \n",
    "    opencv_image = ImageConverter.from_numpy_format(opencv_image).image_in_opencv_format\n",
    "    for i,channel in enumerate(predicted_keypoints):\n",
    "        for keypoint in channel:\n",
    "            color = [0,0,0]\n",
    "            color[i] = 255\n",
    "            color = tuple(color)\n",
    "            cv2.circle(opencv_image,(int(keypoint[0]),int(keypoint[1])),5,color,-1)\n",
    "    cv2.imwrite(f\"detected_keypoints_{image_idx}.png\",opencv_image)\n",
    "    #annotations.append([transform.reverse_transform_point(channel[0]) for channel in predicted_keypoints])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-view triangulation of the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_intrinsics = camera.intrinsics_matrix()\n",
    "view_extrinsics = camera_poses\n",
    "topleft_image_points = [annotation[0] for annotation in annotations]\n",
    "topright_image_points = [annotation[1] for annotation in annotations]\n",
    "bottomleft_image_points = [annotation[2] for annotation in annotations]\n",
    "topleft_midpoint = multiview_triangulation_midpoint(view_extrinsics, [camera_intrinsics]*len(view_extrinsics), topleft_image_points)\n",
    "topright_midpoint = multiview_triangulation_midpoint(view_extrinsics, [camera_intrinsics]*len(view_extrinsics), topright_image_points)\n",
    "bottomleft_midpoint = multiview_triangulation_midpoint(view_extrinsics, [camera_intrinsics]*len(view_extrinsics), bottomleft_image_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{topleft_midpoint=}\")\n",
    "print(f\"{topright_midpoint=}\")\n",
    "print(f\"{bottomleft_midpoint=}\")\n",
    "print(get_triangulation_errors(view_extrinsics, [camera_intrinsics] * len(view_extrinsics), topleft_image_points, topleft_midpoint))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the grasp pose of the microplate in the local frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micropplate_x_axis = topright_midpoint - topleft_midpoint\n",
    "micropplate_x_axis /= np.linalg.norm(micropplate_x_axis)\n",
    "micropplate_y_axis = topleft_midpoint - bottomleft_midpoint\n",
    "micropplate_y_axis /= np.linalg.norm(micropplate_y_axis)\n",
    "micropplate_z_axis = np.cross(micropplate_x_axis,micropplate_y_axis)\n",
    "micropplate_z_axis /= np.linalg.norm(micropplate_z_axis)\n",
    "microplate_position = topleft_midpoint \n",
    "microplate_pose = np.eye(4)\n",
    "microplate_pose[:3,0] = micropplate_x_axis\n",
    "microplate_pose[:3,1] = micropplate_y_axis\n",
    "microplate_pose[:3,2] = micropplate_z_axis\n",
    "microplate_pose[:3,3] = microplate_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grasp in local frame around center and with appropriate orientation.\n",
    "grasp_pose_in_microplate_frame = SE3Container.from_euler_angles_and_translation([np.pi,0,-np.pi/2],[0.06,-0.04,-0.015]).homogeneous_matrix\n",
    "grasp_pose_in_microplate_frame[2,3] -= 0.01 # gripper offset for wide grasp \n",
    "\n",
    "grasp_pose = microplate_pose @ grasp_pose_in_microplate_frame\n",
    "import spatialmath.base as sm\n",
    "grasp_pose = sm.trnorm(grasp_pose)\n",
    "pre_grasp_pose = np.copy(grasp_pose)\n",
    "pre_grasp_pose[2,3] += 0.1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grasp & Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pose = SE3Container.from_euler_angles_and_translation([np.pi,0,0],topleft_midpoint).homogeneous_matrix\n",
    "robot.move_linear_to_tcp_pose(pre_grasp_pose,0.05).wait()\n",
    "gripper.open().wait()\n",
    "robot.move_linear_to_tcp_pose(grasp_pose,0.05).wait()\n",
    "gripper.move(0.075).wait() # cannot close grasp because the forces are too high, even with lowest force..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.move_linear_to_tcp_pose(home_pose,0.1).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microplate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
